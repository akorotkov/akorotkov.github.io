<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Pg | Alexander Korotkov's blog]]></title>
  <link href="http://akorotkov.github.io/blog/categories/pg/atom.xml" rel="self"/>
  <link href="http://akorotkov.github.io/"/>
  <updated>2016-06-17T14:17:15+03:00</updated>
  <id>http://akorotkov.github.io/</id>
  <author>
    <name><![CDATA[Alexander Korotkov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Faceted Search in the Single PostgreSQL Query]]></title>
    <link href="http://akorotkov.github.io/blog/2016/06/17/faceted-search/"/>
    <updated>2016-06-17T14:20:00+03:00</updated>
    <id>http://akorotkov.github.io/blog/2016/06/17/faceted-search</id>
    <content type="html"><![CDATA[<p>Faceted search is very popular buzzword nowadays.  In short, faceted search
specialty is that its results are organized per category.  Popular search
engines are receiving special support of faceted search.</p>

<p>Let’s see what PostgreSQL can do in this field.  At first, let’s formalize our
task.  For each category which have matching documents we want to obtain:</p>

<ul>
  <li>Total number of matching documents;</li>
  <li>TOP N matching documents.</li>
</ul>

<p>For sure, it’s possible to query such data using multiple per category SQL
queries.  But we’ll make it in a single SQL query.  That also would be faster
in majority of cases.  The query below implements faceted search over
PostgreSQL mailing lists archives using window functions and CTE.  Usage
of window function is essential while CTE was used for better query readability.</p>

<p><code>psql Faceted search SQL query
/*
 * Select all matching messages, calculate rank within list and total count
 * within list using window functions.
 */
WITH msg AS (
    SELECT
        message_id,
        subject,
        list,
        RANK() OVER (
            PARTITION BY list
            ORDER BY ts_rank_cd(body_tsvector,  plainto_tsquery('index bloat')), id
        ) rank,
        COUNT(*) OVER (PARTITION BY list) cnt
    FROM messages
    WHERE body_tsvector @@ plainto_tsquery('index bloat')
),
/* Aggregate messages and count per list into json. */
lst AS (
    SELECT
        list,
        jsonb_build_object(
            'count', cnt,
            'results', jsonb_agg(
                jsonb_build_object(
                    'message_id', message_id,
                    'subject', subject
        ))) AS data
    FROM msg
    WHERE rank &lt;= 5
    GROUP by list, cnt
)
/* Aggregate per list data into single json */
SELECT  jsonb_object_agg(list, data)
FROM    lst;
</code></p>

<p>The resulting JSON document contains total count of matching mailing list
messages and TOP 5 relevant messages for each list.</p>

<p><code>js Faceted search JSON result
{
  "pgsql-admin": {
    "count": 263,
    "results": [
      {"message_id": "CACjxUsMUWkY1Z2K2A6yVdF88GT3xcFw5ofWTR6r1zqLUYu0WzA@mail.gmail.com", "subject": "Re: Slow planning time"},
      {"message_id": "dcc563d11001041749w561874f7y6574fb42ab49f850@mail.gmail.com", "subject": "Re: Finetuning Autovacuum"},
      {"message_id": "AANLkTikWabMzCRCSWuNLuPizSSQX3YILgJrNZuzgp3yM@mail.gmail.com", "subject": "Re: blocking automatic vacuum"},
      {"message_id": "dcc563d10904011631l4058aabew12f3fe4895a072f3@mail.gmail.com", "subject": "Re: Vacuum Full"},
      {"message_id": "FE44E0D7EAD2ED4BB2165071DB8E328C03062D9D@egcrc-ex01.egcrc.org", "subject": "Re: postgres bogged down beyond tolerance"
      }
    ]
  },
/*................................................................................*/
  "pgsql-advocacy": {
    "count": 8,
    "results": [
      {"message_id": "Pine.LNX.4.33.0310291602220.22178-100000@css120.ihs.com", "subject": "Re: Press Release"},
      {"message_id": "20050502203626.GA29791@dcc.uchile.cl", "subject": "Re: [HACKERS] Increased company involvement"},
      {"message_id": "5d94f7afb26f56652e06ba0657573ef2@biglumber.com", "subject": "Search and archives still out of sync"},
      {"message_id": "Pine.GSO.4.64.0708010705100.13114@westnet.com", "subject": "Re: postgresql publication"},
      {"message_id": "20070801151739.GF6165@alvh.no-ip.org", "subject": "Re: postgresql publication"
      }
    ]
  }
}
</code></p>

<p>In the plan of this query we can see that <code>message_body_idx</code> GIN index is
scanned only once, and this is great.</p>

<p><code>psql Plan of faceted search SQL query
                                                                       QUERY PLAN
---------------------------------------------------------------------------------------------------------------------------------------------------
 Aggregate  (cost=2369.50..2369.51 rows=1 width=114) (actual time=34.232..34.232 rows=1 loops=1)
   CTE msg
     -&gt;  WindowAgg  (cost=2087.93..2354.30 rows=491 width=336) (actual time=30.925..33.087 rows=2486 loops=1)
           -&gt;  WindowAgg  (cost=2087.93..2222.96 rows=491 width=336) (actual time=30.716..32.020 rows=2486 loops=1)
                 -&gt;  Sort  (cost=2087.93..2089.16 rows=491 width=336) (actual time=30.711..30.838 rows=2486 loops=1)
                       Sort Key: messages.list, (ts_rank_cd(messages.body_tsvector, plainto_tsquery('index bloat'::text))), messages.id
                       Sort Method: quicksort  Memory: 582kB
                       -&gt;  Bitmap Heap Scan on messages  (cost=48.05..2065.98 rows=491 width=336) (actual time=3.037..24.345 rows=2486 loops=1)
                             Recheck Cond: (body_tsvector @@ plainto_tsquery('index bloat'::text))
                             Heap Blocks: exact=2044
                             -&gt;  Bitmap Index Scan on message_body_idx  (cost=0.00..47.93 rows=491 width=0) (actual time=2.723..2.723 rows=2486 loo
                                   Index Cond: (body_tsvector @@ plainto_tsquery('index bloat'::text))
   CTE lst
     -&gt;  HashAggregate  (cost=12.69..13.69 rows=67 width=540) (actual time=34.090..34.133 rows=14 loops=1)
           Group Key: msg.list, msg.cnt
           -&gt;  CTE Scan on msg  (cost=0.00..11.05 rows=164 width=540) (actual time=30.928..33.879 rows=68 loops=1)
                 Filter: (rank &lt;= 5)
                 Rows Removed by Filter: 2418
   -&gt;  CTE Scan on lst  (cost=0.00..1.34 rows=67 width=114) (actual time=34.092..34.140 rows=14 loops=1)
 Planning time: 0.380 ms
 Execution time: 34.357 ms
</code></p>

<p>Thus, it appears that nothing prevents you from implementing trendy kinds of
searches using old good SQL and powerful features of PostgreSQL including:
fulltext search, JSON support, window functions etc.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RuntimeAppend in Pg_pathman: Achievements and New Challenges]]></title>
    <link href="http://akorotkov.github.io/blog/2016/06/15/pg_pathman-runtime-append/"/>
    <updated>2016-06-15T15:00:00+03:00</updated>
    <id>http://akorotkov.github.io/blog/2016/06/15/pg_pathman-runtime-append</id>
    <content type="html"><![CDATA[<p>Dealing with partitioned tables we can’t always select relevant partitions
during query planning.  Naturally, during query planning you can’t know values
which come from subquery or outer part of nested loop join.  Nevertheless, it
would be ridiculous to scan all the partitions in such cases.</p>

<p>This is why my Postgres Professional colleague Dmitry Ivanov developed a
new custom executor node for pg_pathman: RuntimeAppend.  This node behaves
like regular Append node: it contains set of children Nodes which should be
appended.  However, RuntimeAppend have one distinction: each run it selects
only relevant children to append basing on parameter values.</p>

<!--more-->

<p>Let’s consider example: join of <code>journal</code> table which contains row per each
30 seconds of year partitioned by day, and <code>q</code> table which refers 1000 random
rows of <code>journal</code> table.  Without RuntimeAppend optimizer selects Hash Join
plan.</p>

<p><code>sql Regular Append: Hash Join
# EXPLAIN ANALYZE SELECT * FROM q JOIN journal j ON q.dt = j.dt;
                                                          QUERY PLAN
-------------------------------------------------------------------------------------------------------------------------------
 Hash Join  (cost=27.50..25442.51 rows=1000 width=56) (actual time=0.479..252.506 rows=1000 loops=1)
   Hash Cond: (j.dt = q.dt)
   -&gt;  Append  (cost=0.00..21463.01 rows=1051201 width=49) (actual time=0.005..152.258 rows=1051201 loops=1)
         -&gt;  Seq Scan on journal_1 j  (cost=0.00..58.80 rows=2880 width=49) (actual time=0.004..0.247 rows=2880 loops=1)
         -&gt;  Seq Scan on journal_2 j_1  (cost=0.00..58.80 rows=2880 width=49) (actual time=0.001..0.208 rows=2880 loops=1)
         -&gt;  Seq Scan on journal_3 j_2  (cost=0.00..58.80 rows=2880 width=49) (actual time=0.001..0.197 rows=2880 loops=1)
...............................................................................................................................
         -&gt;  Seq Scan on journal_366 j_365  (cost=0.00..1.01 rows=1 width=49) (actual time=0.001..0.001 rows=1 loops=1)
   -&gt;  Hash  (cost=15.00..15.00 rows=1000 width=8) (actual time=0.185..0.185 rows=1000 loops=1)
         Buckets: 1024  Batches: 1  Memory Usage: 48kB
         -&gt;  Seq Scan on q  (cost=0.00..15.00 rows=1000 width=8) (actual time=0.003..0.074 rows=1000 loops=1)
 Planning time: 29.262 ms
 Execution time: 256.337 ms
(374 rows)
</code></p>

<p>The Hash Join execution takes 256 milliseconds for execution and 29 milliseconds
for planning.  Relatively high planning time is expected because all the
partitions are present in plan.  It’s surprising that optimizer didn’t select
Nested Loop join.  Let’s force it to do so by <code>enable_hashjoin = off</code> and
<code>enable_mergejoin = off</code>.</p>

<p><code>sql Regular Append: Nested Loop
# EXPLAIN ANALYZE SELECT * FROM q JOIN journal j ON q.dt = j.dt;
                                                                      QUERY PLAN
------------------------------------------------------------------------------------------------------------------------------------------------------
 Nested Loop  (cost=0.28..170817.00 rows=1000 width=56) (actual time=1.091..452.658 rows=1000 loops=1)
   -&gt;  Seq Scan on q  (cost=0.00..15.00 rows=1000 width=8) (actual time=0.006..0.158 rows=1000 loops=1)
   -&gt;  Append  (cost=0.28..167.14 rows=366 width=49) (actual time=0.218..0.438 rows=1 loops=1000)
         -&gt;  Index Scan using journal_1_dt_idx on journal_1 j  (cost=0.28..0.46 rows=1 width=49) (actual time=0.001..0.001 rows=0 loops=1000)
               Index Cond: (dt = q.dt)
         -&gt;  Index Scan using journal_2_dt_idx on journal_2 j_1  (cost=0.28..0.46 rows=1 width=49) (actual time=0.001..0.001 rows=0 loops=1000)
               Index Cond: (dt = q.dt)
         -&gt;  Index Scan using journal_3_dt_idx on journal_3 j_2  (cost=0.28..0.46 rows=1 width=49) (actual time=0.001..0.001 rows=0 loops=1000)
               Index Cond: (dt = q.dt)
......................................................................................................................................................
         -&gt;  Index Scan using journal_366_dt_idx on journal_366 j_365  (cost=0.12..0.15 rows=1 width=49) (actual time=0.001..0.001 rows=0 loops=1000)
               Index Cond: (dt = q.dt)
 Planning time: 29.922 ms
 Execution time: 456.140 ms
(737 rows)
</code></p>

<p>The Nested Loop join takes 456 milliseconds to execute.  This is even worse.
But this is understandable because we have to scan each partition of <code>journal</code>
for each row of <code>q</code>.</p>

<p>Finally, let’s enable RuntimeAppend.</p>

<p><code>sql RuntimeAppend
# EXPLAIN ANALYZE SELECT * FROM q JOIN journal j ON q.dt = j.dt;
                                                                   QUERY PLAN
------------------------------------------------------------------------------------------------------------------------------------------------
 Nested Loop  (cost=0.28..481.67 rows=1000 width=56) (actual time=0.041..9.911 rows=1000 loops=1)
   -&gt;  Seq Scan on q  (cost=0.00..15.00 rows=1000 width=8) (actual time=0.005..0.079 rows=1000 loops=1)
   -&gt;  Custom Scan (RuntimeAppend)  (cost=0.28..0.46 rows=1 width=49) (actual time=0.003..0.003 rows=1 loops=1000)
         -&gt;  Index Scan using journal_330_dt_idx on journal_330 j  (cost=0.28..0.46 rows=1 width=49) (actual time=0.003..0.003 rows=1 loops=5)
               Index Cond: (dt = q.dt)
         -&gt;  Index Scan using journal_121_dt_idx on journal_121 j  (cost=0.28..0.46 rows=1 width=49) (actual time=0.004..0.004 rows=1 loops=1)
               Index Cond: (dt = q.dt)
         -&gt;  Index Scan using journal_37_dt_idx on journal_37 j  (cost=0.28..0.46 rows=1 width=49) (actual time=0.003..0.003 rows=1 loops=4)
               Index Cond: (dt = q.dt)
................................................................................................................................................
         -&gt;  Index Scan using journal_355_dt_idx on journal_355 j  (cost=0.28..0.46 rows=1 width=49) (actual time=0.003..0.003 rows=1 loops=1)
               Index Cond: (dt = q.dt)
 Planning time: 30.775 ms
 Execution time: 8.615 ms
(687 rows)
</code></p>

<p>The Nested Loop join with RuntimeAppend takes only about 9 milliseconds
to execute!  Such fast execution is possible thanks to RuntimeAppend scans only
one relevant partition of <code>journal</code> for each row of <code>q</code>.</p>

<p>Nevertheless, all the partitions are present in plan and planning time is still
quite high.  This relatively high planning time could be not so significant
for prepared statements or long OLAP queries.</p>

<p>However, long planning time appears to be not the only problem.  We run a
benchmark when RuntimeAppend node returns just a few rows in prepared statement.
Despite high planning time doesn’t affect prepared statements, TPS was few
time slower than it was without partitioning.  After running perf, we got this
<a href="/images/runtimeappend_flamegraph.svg">flamegraph</a>.  This flamegraph shows that
we spend very significant time for locking and unlocking every partition.
Naturally, locking 365 partitions isn’t using fast-path locking and appears to
be significant overhead.</p>

<p>Thus, we see how huge benefit could runtime partition selection have.  However,
in current design having all the partitions in plan cause high overhead.
Solution could be found in redesigning partition locking.  We are researching
this problem now.  It’s likely this problem can’t be solved in the boundaries
of extension and proper solution requires hacking of PostgreSQL core.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drawing Graphs Directly in Psql]]></title>
    <link href="http://akorotkov.github.io/blog/2016/06/09/psql-graph/"/>
    <updated>2016-06-09T16:45:00+03:00</updated>
    <id>http://akorotkov.github.io/blog/2016/06/09/psql-graph</id>
    <content type="html"><![CDATA[<p>For people who are actively working with psql, it frequently happens that you
want to draw graph for the table you’re currently seeing.  Typically, it means a
cycle of actions including: exporting data, importing it into graph drawing tool
and drawing graph itself.  It appears that this process could be automated:
graph could be drawn by typing a single command directly in psql.  See an
example on the screenshot below.</p>

<p><img class="no-border center" src="/images/screen-psql-iterm-graph.png" width="689" height="958"></p>

<!--more-->

<p>It might seem like a magic, but actually there is absolutely no magic.  iTerm2
supports <a href="https://www.iterm2.com/documentation-images.html">image inlining</a>
since version 3 which is currently beta.  Thus, if we put image surrounded
with corresponding escape sequences it will appear in the terminal.  From psql
side we need to redirect output to the script which would do it.  We can define
a macro for simplifying this like in <a href="/blog/2015/08/26/psql-gdb-attach/">one of my previous posts</a>.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="err">\</span><span class="k">set</span> <span class="n">graph</span> <span class="err">‘\</span><span class="k">g</span> <span class="o">|</span><span class="n">pg_graph</span><span class="err">’</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>And finally we need a pg_graph script which parses psql output, draws graph and
puts it into stdout.  I <a href="https://gist.github.com/akorotkov/2c7011ac30de27b9c5631f1bf418f0a1">wrote one</a>
using Python and matplotlib.  It recognizes first column as series of X-values
and rest of columns as series of Y-values.  If first column contains only
decimal values it draws a plot chart, otherwise it draws a bar chart.</p>

<p>Thereby, it’s not hard to teach psql to do more things.  Also, we can consider
some improvements to psql including:</p>

<ul>
  <li>Add output format option for <code>\g</code> which would make it easier to parse psql
output from scripts;</li>
  <li>Provide elegant way to pass parameters into psql macro.</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PostgreSQL Scalability: Towards Millions TPS]]></title>
    <link href="http://akorotkov.github.io/blog/2016/05/09/scalability-towards-millions-tps/"/>
    <updated>2016-05-09T12:50:00+03:00</updated>
    <id>http://akorotkov.github.io/blog/2016/05/09/scalability-towards-millions-tps</id>
    <content type="html"><![CDATA[<p>PostgreSQL scalability on multicore and multisocket machines became a subject
of optimization long time ago once such machines became widely used.
<a href="http://suckit.blog.hu/2009/09/29/postgresql_history">This blog post</a> shows
brief history of vertical scalability improvements between versions 8.0 and 8.4.
PostgreSQL 9.2 had very noticeable scalability improvement.  Thanks to fast
path locking and other optimizations it becomes possible to achieve
<a href="http://rhaas.blogspot.ru/2012/04/did-i-say-32-cores-how-about-64.html">more than 350 000 TPS in select-only pgbench test</a>.  The latest stable release PostgreSQL 9.5 also
contain significant scalability advancements including LWLock improvement which
allows achieving <a href="http://amitkapila16.blogspot.ru/2015/01/read-scalability-in-postgresql-95.html">about 400 000 TPS in select-only pgbench test</a>.</p>

<p>Postgres Professional company also became involved into scalability
optimization.  In partnership with IBM we researched PostgreSQL scalability on
modern Power8 servers.  The results of this research was published in
<a href="https://habrahabr.ru/company/postgrespro/blog/270827/">popular Russian blog habrahabr</a>
(<a href="https://translate.google.com/translate?sl=ru&amp;tl=en&amp;js=y&amp;prev=_t&amp;hl=ru&amp;ie=UTF-8&amp;u=https%3A%2F%2Fhabrahabr.ru%2Fcompany%2Fpostgrespro%2Fblog%2F270827%2F&amp;edit-text=&amp;act=url">Google translated version</a>).
As brief result of this research we identify two ways to improve PostgreSQL
scalability on Power8:</p>

<ol>
  <li>Implement Pin/UnpinBuffer() using CAS operations instead of
buffer header spinlock;</li>
  <li>Optimize LWLockAttemptLock() in assembly to make fewer loops for changing
lwlock state.</li>
</ol>

<p>The optimization #1 appears to give huge benefit on big Intel servers as well,
while optimization #2 is Power-specific.  After long rounds of optimization,
cleaning and testing #1 was finally
<a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=48354581">committed by Andres Freund</a>.</p>

<p><img class="no-border center 2x" src="/images/scalability.png" width="720" height="576"></p>

<!--more-->

<p>On the graph above, following PostgreSQL versions were compared:</p>

<ol>
  <li>9.5.2 release – peak is 540 000 TPS with 60 clients,</li>
  <li>9.6 master (more precisely <a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=59455018">59455018</a>) – peak is 1 064 000 TPS
with 110 clients,</li>
  <li>9.6 master where all PGXACTs were full cacheline aligned – peak is
1 722 000 TPS with 200 clients.</li>
</ol>

<p>Alignment issues worth some explanation.  Initially, I complained performance
regression introduced by commit <a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=5364b357">5364b357</a> which increases number of
clog buffers.  That was strange by itself, because read-only benchmark shouldn’t
lookup to clog thanks to hint bits.  As expected it appears that clog buffers
don’t really affect read-only performance directly, <a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=5364b357">5364b357</a> just
changed layout of shared memory structures.</p>

<p>It appears that read-only benchmark became very sensitive to layout of shared
memory structures. As result performance has significant variety depending on
shared_buffers, max_connections and other options which influence shared memory
distribution.  When I gave Andres access to that big machine, he very quickly
find a way to take care about performance irregularity:
<a href="http://www.postgresql.org/message-id/20160411214029.ce3fw6zxim5k6a2r@alap3.anarazel.de">make all PGXACTs full cacheline aligned</a>.
Without this patch SnapshotResetXmin() dirties processor cache containing
multiple PGXACTs.  With this patch SnapshotResetXmin() dirties cacheline with
only single PGXACT.  Thus, GetSnapshotData() have much less cache misses.
That was surprising and good lesson for me.  I knew that alignment influence
performance, but I didn’t expect this influence to be so huge. PGXACT cacheline
alignment issue was discovered after feature freeze for 9.6.  That
means it would be subject for 9.7 development.  Nevertheless, 9.6 have very
noticeable scalability improvement.</p>

<p>Therefore, PostgreSQL single instance delivers more than 1 million TPS and one
could say, that PostgreSQL opens a new era of millions TPS.</p>

<p>P.S. I’d like to thank:</p>

<ul>
  <li>Andres Freund, so-author and committer of patch;</li>
  <li>My PostgresPro colleagues: Dmitry Vasilyev who run a lot of benchmarks,
YUriy Zhuravlev who wrote original proof of concept of this patch;</li>
  <li>Dilip Kumar and Robert Haas who helped with testing.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Extensible Access Methods Are Committed to 9.6]]></title>
    <link href="http://akorotkov.github.io/blog/2016/04/06/extensible-access-methods/"/>
    <updated>2016-04-06T15:26:00+03:00</updated>
    <id>http://akorotkov.github.io/blog/2016/04/06/extensible-access-methods</id>
    <content type="html"><![CDATA[<p>PostgreSQL 9.6 receives suitable support of extensible index access methods.
And that’s good news because Postgres was initially designed to support it.</p>

<blockquote>
  <p>“It is imperative that a user be able to construct new access methods to 
provide efficient access to instances of nontraditional base types”</p>

  <p>Michael Stonebraker, Jeff Anton, Michael Hirohama.
Extendability in POSTGRES , IEEE Data Eng. Bull. 10 (2) pp.16-23, 1987</p>
</blockquote>

<p>That was a huge work which consists of multiple steps.</p>

<!--more-->

<ol>
  <li>Rework access method interface so that access method internals are hidden
from SQL level to C level.  Besides help for custom access methods support,
this refactoring is good by itself.<br />
<a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=65c5fcd3">Committed</a> by Tom Lane.</li>
  <li><code>CREATE ACCESS METHOD</code> command which provides legal way for insertion into
pg_am with support of dependencies and pg_dump/pg_restore.
<a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=473b9328">Committed</a> by Alvaro Herrera.</li>
  <li>Generic WAL interface which provides custom access methods the way to be
WAL-logged.  Each built-in access method has its own type of WAL records.
But custom access method shouldn’t because it could affect reliability.
Generic WAL records represent difference between pages in general way as
result of per-byte comparison of original and modified images of the page.
For sure, it is not as efficient as own type of WAL records, but there is
no choice under restrictions we have.
<a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=65578341">Committed</a> by Teodor Sigaev.</li>
  <li>Bloom contrib module which is example of custom index access method which
uses generic WAL interface.  This contrib is essential for testing
infrastructure described above.  Also, this access method could be useful by
itself.
<a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=9ee014fc">Committed</a> by Teodor Sigaev.</li>
</ol>

<p>I am very thankful for the efforts of committers and reviewers who make it
possible to include these features into PostgreSQL.</p>

<p>However, end users don’t really care about this infrastructure.  They do care
about features we can provide on the base of this infrastructure.  Actually,
we would be able to have index access methods which are:</p>

<ul>
  <li>Too hard to add to PostgreSQL core.  For instance, we presented
<a href="https://wiki.postgresql.org/images/2/25/Full-text_search_in_PostgreSQL_in_milliseconds-extended-version.pdf">fast FTS</a>
in 2012.  We have 2 of 4 GIN features committed to core.  And it seems to be
very long way to have rest of features in core.  But since 9.6 we would
provide it as an extension.</li>
  <li>Not patent free.  There are some interesting data structures which are
covered by patents (Fractal Tree index, for example).  This is why they
couldn’t be added to PostgreSQL core.  Since 9.6, they could be provided
without fork.</li>
</ul>

<p>Also, I consider this work as an approach (together with FDW) to pluggable
storage engines.  I will speak about this during my
<a href="http://www.pgcon.org/2016/schedule/events/920.en.html">talk at PGCon 2016</a>.</p>

]]></content>
  </entry>
  
</feed>
